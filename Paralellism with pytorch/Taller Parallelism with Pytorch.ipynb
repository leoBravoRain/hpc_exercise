{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop\n",
    "\n",
    "1) Introduction to Pytorch (Tensors)\n",
    "\n",
    "2) Pytorch and GPU\n",
    "\n",
    "3) Build a NN with Pytorch\n",
    "\n",
    "4) Train NN sequentially\n",
    "\n",
    "5) Train NN in GPU\n",
    "\n",
    "5) Train NN with multiples GPUs (Data Parallelism and Model Parallelism)\n",
    "\n",
    "6) Introduction to Multiprocessing with Python\n",
    "\n",
    "7) Train NN using Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instal Pytorch and torchvision: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)torchvision\n",
    "\n",
    "2)torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.multiprocessing as mp \n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# Multiprocessing Python\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to [Pytorch](https://pytorch.org/tutorials/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 5.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2])\n",
    "\n",
    "b = torch.Tensor([2, 3])\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to GPU in Pytorch [(documentation)](https://pytorch.org/docs/stable/cuda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1070'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce GTX 1070', major=6, minor=1, total_memory=8119MB, multi_processor_count=16)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the current GPU memory usage by \n",
    "# tensors in bytes for a given device\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the current GPU memory managed by the\n",
    "# caching allocator in bytes for a given device\n",
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releases all unoccupied cached memory currently held by\n",
    "# the caching allocator so that those can be used in other\n",
    "# GPU application and visible in nvidia-smi\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Select device (CUDA or CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 5.])\n"
     ]
    }
   ],
   "source": [
    "# CPU storage\n",
    "a = torch.Tensor([1., 2.])\n",
    "\n",
    "# CPU storage\n",
    "b = torch.Tensor([2, 3])\n",
    "\n",
    "# CPU storage\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ec4c3a33de5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# Move to GPU\n",
    "\n",
    "a = a.to(device)\n",
    "a = a.to('cuda')\n",
    "\n",
    "print(a.get_device())\n",
    "print(b.get_device())\n",
    "\n",
    "b = b.to(device)\n",
    "d = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN for image classification using Pytorch (CPU, GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_data = torchvision.datasets.MNIST('dataset', \n",
    "                                        train=True, \n",
    "                                        download=True, \n",
    "                                        transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Get test data\n",
    "test_data = torchvision.datasets.MNIST('dataset', \n",
    "                                        train = False, \n",
    "                                        download = True,\n",
    "                                        transform = torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f355498e748>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display an example\n",
    "data = train_data[random.randint(0, len(train_data))]\n",
    "\n",
    "print('Label: ', data[1])\n",
    "\n",
    "image = data[0]\n",
    "\n",
    "# plotting\n",
    "plt.imshow(image.numpy()[0, :, :], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crate data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training loader\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = 32)\n",
    "\n",
    "# Get testing loader\n",
    "test_loader = DataLoader(test_data, shuffle = False, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional network (Intento de LeNet)\n",
    "class conv_net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(conv_net, self).__init__()\n",
    "        \n",
    "        # First layer\n",
    "        self.conv1 = torch.nn.Conv2d(1, 60, kernel_size = 5, stride = 1, padding = 2, bias = True) \n",
    "        self.avgpooling = torch.nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # Activation layer\n",
    "        self.act = torch.nn.Tanh()\n",
    "        \n",
    "        # Second layer\n",
    "        self.conv2 = torch.nn.Conv2d(60, 16*2, kernel_size = 5, stride = 1, padding = 0, bias = True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fcc = torch.nn.Linear((16*2)*5*5, 120, bias = True)\n",
    "        self.fc = torch.nn.Linear(120, 84, bias = True)\n",
    "        \n",
    "        # Outputlayer\n",
    "        self.soft_fc = torch.nn.Sequential(\n",
    "        \n",
    "            torch.nn.Linear(84, 10, bias = True),\n",
    "            torch.nn.Softmax()\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        input_size = x.size(0)\n",
    "        \n",
    "        # First conv layer\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.avgpooling(x)\n",
    "        \n",
    "        # Second conv layer\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.avgpooling(x)\n",
    "        \n",
    "        # Flatting layer\n",
    "        x = x.view(input_size, -1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.act(self.fcc(x))\n",
    "        x = self.act(self.fc(x))\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.soft_fc(x)\n",
    " \n",
    "        # Return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID  15136\n",
      "Epoch  0\n",
      "Epoch  1\n",
      "Sequential trainig time: 94.42112231254578 [sec]\n"
     ]
    }
   ],
   "source": [
    "# epochs = 2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('PID ', os.getpid())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('Epoch ', epoch)\n",
    "    \n",
    "    # Training\n",
    "    for data, label in train_loader:\n",
    "        \n",
    "        # make predictions\n",
    "        prediction = model(data)\n",
    "        \n",
    "        # zero gradient = not acumulating value\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # get loss value\n",
    "        loss = criterion(prediction, label)  \n",
    "                  \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "print('Sequential trainig time: {0} [sec]'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_net(\n",
      "  (conv1): Conv2d(1, 60, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (avgpooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (act): Tanh()\n",
      "  (conv2): Conv2d(60, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fcc): Linear(in_features=800, out_features=120, bias=True)\n",
      "  (fc): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (soft_fc): Sequential(\n",
      "    (0): Linear(in_features=84, out_features=10, bias=True)\n",
      "    (1): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = model_seq\n",
    "\n",
    "# model = model_mp\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without gradient\n",
    "with torch.no_grad():\n",
    "    \n",
    "    predictions = np.empty(len(test_data))\n",
    "    labels = np.empty(len(test_data))\n",
    "    \n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    \n",
    "    # For compute accuracy\n",
    "    total = 0\n",
    "    \n",
    "    # minibatches for validation dataset\n",
    "    for data, label in test_loader:\n",
    "        \n",
    "        # get prediction ussing trained model\n",
    "        prediction = model(data)\n",
    "       \n",
    "        loss = criterion(prediction, label) \n",
    "        \n",
    "        # get prediction (max of prediction vector)\n",
    "        _, predicted = torch.max(prediction.data, 1)\n",
    "        \n",
    "        predictions[total: total + 256] = predicted.numpy()\n",
    "        \n",
    "        labels[total: total + 256] = label\n",
    "        \n",
    "        # Get total of instances\n",
    "        total += label.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.06\n",
      "accuracy score: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_score = f1_score(labels, predictions, average='macro')\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "print(\"f1 score: {0:.2f}\".format(f1_score))\n",
    "print('accuracy score: {0:.2f}'.format(accuracy_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model in GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has tensors (Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight <class 'torch.Tensor'>\n",
      "conv1.bias <class 'torch.Tensor'>\n",
      "conv2.weight <class 'torch.Tensor'>\n",
      "conv2.bias <class 'torch.Tensor'>\n",
      "fcc.weight <class 'torch.Tensor'>\n",
      "fcc.bias <class 'torch.Tensor'>\n",
      "fc.weight <class 'torch.Tensor'>\n",
      "fc.bias <class 'torch.Tensor'>\n",
      "soft_fc.0.weight <class 'torch.Tensor'>\n",
      "soft_fc.0.bias <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    \n",
    "#     if param.requires_grad:\n",
    "        \n",
    "    print(name, type(param.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "GPU trainig time: 20.047572135925293 [sec]\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    \n",
    "    # Allocate model in GPU\n",
    "    model = model.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# epochs = 2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# print('PID ', os.getpid())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('Epoch ', epoch)\n",
    "    \n",
    "    # Training\n",
    "    for data, label in train_loader:\n",
    "        \n",
    "        # If GPU\n",
    "        if use_gpu:\n",
    "            \n",
    "            data, label = data.to('cuda'), label.to('cuda')\n",
    "            \n",
    "        # make predictions\n",
    "        prediction = model(data)\n",
    "        \n",
    "        # zero gradient = not acumulating value\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # get loss value\n",
    "        loss = criterion(prediction, label)  \n",
    "                  \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "if use_gpu:\n",
    "    \n",
    "    model = model.to('cpu')\n",
    "    \n",
    "print('GPU trainig time: {0} [sec]'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model in multiple GPU (Data Parallelism) [link](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1  GPU :( !\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-193cd5ec2149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# To GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if it has more than 1 GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    \n",
    "    print(\"Let's use\", torch.cuda.device_count(), \" GPUs!\")\n",
    "    \n",
    "    # It will be parallelized over multiple GPUs in the batch dimension\n",
    "    # Data Parallelism is when we split the mini-batch of samples \n",
    "    # into multiple smaller mini-batches and run the computation \n",
    "    # for each of the smaller mini-batches in parallel.\n",
    "    \n",
    "    # DataParallel splits your data automatically\n",
    "    # and sends job orders to multiple models on \n",
    "    # several GPUs. After each model finishes their job, \n",
    "    # DataParallel collects and merges the results before returning it to you.\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "else:\n",
    "    \n",
    "    print(\"Let's use\", torch.cuda.device_count(), \" GPU :( !\")\n",
    "    \n",
    "# To GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2a88b15b3b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# zero gradient = not acumulating value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b26bfe4fdfc9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# First conv layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# if use_gpu:\n",
    "    \n",
    "#     # Allocate model in GPU\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# epochs = 2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# print('PID ', os.getpid())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('Epoch ', epoch)\n",
    "    \n",
    "    # Training\n",
    "    for data, label in train_loader:\n",
    "        \n",
    "        # If GPU\n",
    "        if use_gpu:\n",
    "            \n",
    "            data, label = data.to('cuda'), label.to('cuda')\n",
    "            \n",
    "        # make predictions\n",
    "        prediction = model(data)\n",
    "        \n",
    "        # zero gradient = not acumulating value\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # get loss value\n",
    "        loss = criterion(prediction, label)  \n",
    "                  \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "if use_gpu:\n",
    "    \n",
    "    model = model.to('cpu')\n",
    "    \n",
    "print('GPU trainig time: {0} [sec]'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parallelism [link](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)\n",
    "\n",
    "What when models are too big to fit in 1 GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiples GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.net1(x.to('cuda:0')))\n",
    "        \n",
    "        return self.net2(x.to('cuda:1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CPU_GPU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(10, 100)\n",
    "        self.layer_2 = torch.nn.Linear(100,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Compute on CPU\n",
    "        x = self.layer_1(x)\n",
    "        \n",
    "        # Transfer to GPU\n",
    "        x = x.cuda(0)\n",
    "\n",
    "        # Computeon GPU\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class Process (one task to one process)\n",
    "- Class Pool (pool of workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID:  6230\n",
      "PID:  6233\n",
      "PID:  6236\n",
      "PID:  6239\n"
     ]
    }
   ],
   "source": [
    "# function \n",
    "def func(rank):\n",
    "    \n",
    "    print('PID: ', os.getpid())\n",
    "    \n",
    "#     time.sleep(rank)\n",
    "    \n",
    "#     print('Exiting PID ', os.getpid())\n",
    "    \n",
    "# Like main method\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Get number of processes\n",
    "    num_processes = 4\n",
    "    \n",
    "    # List of processes\n",
    "    processes = []\n",
    "    \n",
    "    # Create process\n",
    "    for rank in range(num_processes):\n",
    "        \n",
    "        p = Process(target = func, args = (rank, ))\n",
    "         \n",
    "        # Start process\n",
    "        p.start()\n",
    "        \n",
    "        # add process\n",
    "        processes.append(p)\n",
    "        \n",
    "    # Wait until all processes finish\n",
    "    for p in processes:\n",
    "        \n",
    "        # Wait process finish\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function for MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It mus to run from zero because it fails if try to run after other processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \n",
    "    print('Train function')\n",
    "    \n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=32, num_workers = 1)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    print('PID: ', pid)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch ', epoch)\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for data, label in train_loader:\n",
    "        \n",
    "            print('new train')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = F.nll_loss(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "#             if batch_idx % 100 == 0:\n",
    "                \n",
    "#             print(batch_idx)\n",
    "#                 print('PID {} iteration {}'.format(pid, batch_idx))\n",
    "\n",
    "    \n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train function\n",
      "PID:  16178\n",
      "Epoch  0\n",
      "Train function\n",
      "PID:  16181\n",
      "Epoch  0\n",
      "new train\n",
      "new train\n",
      "MP time: 0.2648591995239258 [sec]\n"
     ]
    }
   ],
   "source": [
    "# Main method\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "#     mp.set_start_method('spawn') # it must to run only one time\n",
    "    \n",
    "    model = conv_net()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.share_memory()\n",
    "    \n",
    "    processes = []\n",
    "    \n",
    "    num_process = 2\n",
    "    \n",
    "    for rank in range(num_process):\n",
    "        \n",
    "        p = mp.Process(target = train, args = (model, ))\n",
    "        \n",
    "        p.start()\n",
    "        \n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        \n",
    "        p.join()\n",
    "        \n",
    "    print('MP time: {} [sec]'.format(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
